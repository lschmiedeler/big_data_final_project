{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ee2029-5f4c-414f-a36a-04476fc87f82",
   "metadata": {
    "id": "a2ee2029-5f4c-414f-a36a-04476fc87f82"
   },
   "source": [
    "# DSCI 617 Final Project\n",
    "## Lauren Schmiedeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a9b34-a93a-466d-a426-df4ed03f23b6",
   "metadata": {
    "id": "854a9b34-a93a-466d-a426-df4ed03f23b6"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# create a function that returns the time elapsed given an initial time\n",
    "def find_time_elapsed(t0):\n",
    "    # find the time elapsed in minutes\n",
    "    time_elapsed = (time.time() - t0) / 60\n",
    "    units = \"minutes\"\n",
    "    # convert the time elapsed to hours if time elapsed >= 1 hour\n",
    "    if time_elapsed >= 60:\n",
    "        time_elapsed = time_elapsed / 60\n",
    "        units = \"hours\"\n",
    "    # return a string that tells how much time has elapsed since the initial time\n",
    "    return \"time elapsed = \" + str(round(time_elapsed, 5)) + \" \" + units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WPxBTJqlhISg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPxBTJqlhISg",
    "outputId": "7e41bcbc-d2d7-431d-8d99-148dcc517de9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# mount Google Drive to import the data files using Google Colab\n",
    "# Google Colab handles hyperparameter tuning on a large dataset better than my laptop\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da1c08-5022-4a04-ba81-605bc4763e72",
   "metadata": {
    "id": "63da1c08-5022-4a04-ba81-605bc4763e72"
   },
   "source": [
    "### 1. Read 12 monthly datasets, `chicago_taxi_trips_2016_01`, `chicago_taxi_trips_2016_02`, …, `chicago_taxi_trips_2016_12`, into a Spark DataFrame (Hint: You can use `df = spark.read.csv('C:\\\\Users\\\\yliu3\\\\Documents\\\\Data Banks\\\\Regression\\\\chicago-taxi-rides-2016\\\\*.csv', header = True, inferSchema = True`) to read several csv files and combine them into a single DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X_Q1zVWrhYZI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_Q1zVWrhYZI",
    "outputId": "822d79db-da0a-41b5-bdb0-b4e2d62501f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 43 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 56.4 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=c4b7d48ca72c4cca22b9b404a9b2895d321724251d89902e6466e9ce88a19c2c\n",
      "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
     ]
    }
   ],
   "source": [
    "# install findspark and pyspark in Google Colab\n",
    "!pip install findspark\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a301269-c7fe-4a00-9a0a-6aafde0b3f4e",
   "metadata": {
    "id": "5a301269-c7fe-4a00-9a0a-6aafde0b3f4e"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Final Project\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "seed = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6dfca9-13be-4d2d-9a2b-b8cc8279c21d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d6dfca9-13be-4d2d-9a2b-b8cc8279c21d",
    "outputId": "5c0e8646-f27a-4419-f32a-e91a549c6a16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+-----+----+-----+------+----------+------------+-------+---------------+----------------+----------------+-----------------+\n",
      "|taxi_id|trip_start_timestamp| trip_end_timestamp|trip_seconds|trip_miles|pickup_census_tract|dropoff_census_tract|pickup_community_area|dropoff_community_area| fare|tips|tolls|extras|trip_total|payment_type|company|pickup_latitude|pickup_longitude|dropoff_latitude|dropoff_longitude|\n",
      "+-------+--------------------+-------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+-----+----+-----+------+----------+------------+-------+---------------+----------------+----------------+-----------------+\n",
      "|     85| 2016-01-13 06:15:00|2016-01-13 06:15:00|         180|       0.4|               null|                null|                   24|                    24|  4.5| 0.0|  0.0|   0.0|       4.5|        Cash|    107|            199|             510|             199|              510|\n",
      "|   2776| 2016-01-22 09:30:00|2016-01-22 09:45:00|         240|       0.7|               null|                null|                 null|                  null| 4.45|4.45|  0.0|   0.0|       8.9| Credit Card|   null|           null|            null|            null|             null|\n",
      "|   3168| 2016-01-31 21:30:00|2016-01-31 21:30:00|           0|       0.0|               null|                null|                 null|                  null|42.75| 5.0|  0.0|   0.0|     47.75| Credit Card|    119|           null|            null|            null|             null|\n",
      "|   4237| 2016-01-23 17:30:00|2016-01-23 17:30:00|         480|       1.1|               null|                null|                    6|                     6|  7.0| 0.0|  0.0|   0.0|       7.0|        Cash|   null|            686|             500|             686|              500|\n",
      "|   5710| 2016-01-14 05:45:00|2016-01-14 06:00:00|         480|      2.71|               null|                null|                   32|                  null|10.25| 0.0|  0.0|   0.0|     10.25|        Cash|   null|            385|             478|            null|             null|\n",
      "|   1987| 2016-01-08 18:15:00|2016-01-08 18:45:00|        1080|       6.2|               null|                null|                    8|                     3|17.75| 0.0|  0.0|   0.0|     17.75|        Cash|   null|            599|             346|             660|              120|\n",
      "|   4986| 2016-01-14 04:30:00|2016-01-14 05:00:00|        1500|      18.4|               null|                null|                 null|                  null| 45.0|12.0|  0.0|   0.0|      57.0| Credit Card|   null|           null|            null|            null|             null|\n",
      "|   6400| 2016-01-26 04:15:00|2016-01-26 04:15:00|          60|       0.2|               null|                null|                   16|                    16| 3.75| 0.0|  0.0|   0.0|      3.75|        Cash|    107|            527|              24|             527|               24|\n",
      "|   7418| 2016-01-22 11:30:00|2016-01-22 11:45:00|         180|       0.0|               null|                 504|                    8|                    32|  5.0| 2.0|  0.0|   1.5|       8.5| Credit Card|     82|            210|             470|             744|              605|\n",
      "|   6450| 2016-01-07 21:15:00|2016-01-07 21:15:00|           0|       0.0|               null|                null|                 null|                  null| 3.25| 0.0|  0.0|   1.5|      4.75|        Cash|   null|           null|            null|            null|             null|\n",
      "+-------+--------------------+-------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+-----+----+-----+------+----------+------------+-------+---------------+----------------+----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "\n",
      "number of rows = 19866157 \n",
      "\n",
      "time elapsed = 2.2301 minutes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "t0_read_data = time.time()\n",
    "\n",
    "# first, create a list of paths to the monthly csv files\n",
    "paths = []\n",
    "for file in os.listdir(\"drive/MyDrive/data\"):\n",
    "    if re.search(\"chicago_taxi_trips_2016_[0-9]{2}.csv\", file):\n",
    "        paths.append(\"drive/MyDrive/data/\" + file)\n",
    "\n",
    "# next, read in all the monthly csv files\n",
    "df = spark.read.csv(paths, header = True, inferSchema = True)\n",
    "print(df.show(10))\n",
    "print(\"\\nnumber of rows =\", df.count(), \"\\n\")\n",
    "\n",
    "print(find_time_elapsed(t0_read_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3997f-d60f-4f7e-bbd0-c3b2d5657182",
   "metadata": {
    "id": "cae3997f-d60f-4f7e-bbd0-c3b2d5657182"
   },
   "source": [
    "### 2. There are some missing values.  Please drop the NAs using `df = df.na.drop()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5c464-6645-4d00-b2fe-e782c3481d5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ac5c464-6645-4d00-b2fe-e782c3481d5e",
    "outputId": "a7c022e1-85ce-44d6-cf3f-b82a704c6019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----------+\n",
      "| fare|trip_seconds|trip_miles|\n",
      "+-----+------------+----------+\n",
      "|  4.5|         180|       0.4|\n",
      "| 4.45|         240|       0.7|\n",
      "|42.75|           0|       0.0|\n",
      "|  7.0|         480|       1.1|\n",
      "|10.25|         480|      2.71|\n",
      "|17.75|        1080|       6.2|\n",
      "| 45.0|        1500|      18.4|\n",
      "| 3.75|          60|       0.2|\n",
      "|  5.0|         180|       0.0|\n",
      "| 3.25|           0|       0.0|\n",
      "+-----+------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first, select only the \"fare\", \"trip_seconds\", and \"trip_miles\" columns\n",
    "df = df.select(\"fare\", \"trip_seconds\", \"trip_miles\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f256235-aa6f-4c77-962d-a1ecab405942",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f256235-aa6f-4c77-962d-a1ecab405942",
    "outputId": "282ad963-073b-4114-a448-3c350b6f6d7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows = 19862606\n"
     ]
    }
   ],
   "source": [
    "# next, drop the rows with NAs in the remaining columns\n",
    "df = df.na.drop()\n",
    "print(\"number of rows =\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4w7w-AdKDJIV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4w7w-AdKDJIV",
    "outputId": "883a921c-d6b7-4b04-fb33-1f4c64b6bbcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+\n",
      "|summary|              fare|     trip_seconds|        trip_miles|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|  count|          19862606|         19862606|          19862606|\n",
      "|   mean|13.892086664760871|767.0163579240307| 3.394684370219944|\n",
      "| stddev|25.385934033731534|1060.416563035996|22.597176756854346|\n",
      "|    min|               0.0|                0|               0.0|\n",
      "|    25%|              6.25|              300|               0.1|\n",
      "|    50%|               8.5|              540|               1.1|\n",
      "|    75%|             14.25|              900|               2.7|\n",
      "|    max|            9999.0|            86399|            3353.1|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summarize the data\n",
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c38d0-9816-424f-a6fd-7ad7385764bc",
   "metadata": {
    "id": "8a0c38d0-9816-424f-a6fd-7ad7385764bc"
   },
   "source": [
    "### 3. You are asked to forecast `fare` using `trip_seconds`, `trip_miles` and build a linear regression with elastic net regularizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650e0aa-a766-49a6-89d0-16ae33078d79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f650e0aa-a766-49a6-89d0-16ae33078d79",
    "outputId": "32bd316b-7a0e-48c2-a567-96abef9bee5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----------+-------------+\n",
      "|label|trip_seconds|trip_miles|     features|\n",
      "+-----+------------+----------+-------------+\n",
      "|  4.5|         180|       0.4|  [180.0,0.4]|\n",
      "| 4.45|         240|       0.7|  [240.0,0.7]|\n",
      "|42.75|           0|       0.0|    (2,[],[])|\n",
      "|  7.0|         480|       1.1|  [480.0,1.1]|\n",
      "|10.25|         480|      2.71| [480.0,2.71]|\n",
      "|17.75|        1080|       6.2| [1080.0,6.2]|\n",
      "| 45.0|        1500|      18.4|[1500.0,18.4]|\n",
      "| 3.75|          60|       0.2|   [60.0,0.2]|\n",
      "|  5.0|         180|       0.0|  [180.0,0.0]|\n",
      "| 3.25|           0|       0.0|    (2,[],[])|\n",
      "+-----+------------+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# first, assemble the features (\"trip_seconds\" and \"trip_miles\") into a \"features\" column using VectorAssembler\n",
    "assembler = VectorAssembler(inputCols = [\"trip_seconds\", \"trip_miles\"], outputCol = \"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# second, rename the \"fare\" column to \"label\"\n",
    "df = df.withColumnRenamed(\"fare\", \"label\")\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c331c68-9e50-42c0-b27f-2d246f2d7b65",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c331c68-9e50-42c0-b27f-2d246f2d7b65",
    "outputId": "d4cf139c-92a4-425a-dfe6-65b8e0ccbd3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of observations in the training set = 13902282\n",
      "number of observations in the test set = 5960324 \n",
      "\n",
      "time elapsed = 3.13004 minutes\n"
     ]
    }
   ],
   "source": [
    "t0_split = time.time()\n",
    "\n",
    "# next, split the data into training and test sets\n",
    "(train, test) = df.randomSplit([0.7, 0.3], seed = seed)\n",
    "print(\"number of observations in the training set =\", train.count())\n",
    "print(\"number of observations in the test set =\", test.count(), \"\\n\")\n",
    "\n",
    "print(find_time_elapsed(t0_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395cf21a-af3c-4493-b1f5-74c0d872d50f",
   "metadata": {
    "id": "395cf21a-af3c-4493-b1f5-74c0d872d50f"
   },
   "outputs": [],
   "source": [
    "# create a function that fits the given model on the training data and returns the predictions made on the test data\n",
    "def fit_and_predict(model, train, test):\n",
    "    # find the initial time\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # fit the model on the training data\n",
    "    model_fit = model.fit(train)\n",
    "    \n",
    "    # make predictions on the test data\n",
    "    model_predict = model_fit.transform(test).select(\"prediction\", \"label\")\n",
    "    \n",
    "    # print the time elapsed\n",
    "    print(find_time_elapsed(t0))\n",
    "    \n",
    "    # return the predictions made on the test data\n",
    "    return model_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad8ea1e-9157-496a-ade3-31877a421372",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ad8ea1e-9157-496a-ade3-31877a421372",
    "outputId": "ffda0b8c-99bd-4b47-f309-12a60083f19c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed = 3.34559 minutes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lrPredictions = fit_and_predict(LinearRegression(elasticNetParam = 0.5), train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f1483-e1da-4cb1-9a7f-8b91b65a8f51",
   "metadata": {
    "id": "2c6f1483-e1da-4cb1-9a7f-8b91b65a8f51"
   },
   "source": [
    "### 4. You are asked to forecast `fare` using `trip_seconds`, `trip_miles` and build a simple tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27abc00c-9ae7-4be9-bb85-5bd2c65016d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27abc00c-9ae7-4be9-bb85-5bd2c65016d1",
    "outputId": "68775c93-ccd9-4da4-a0f3-4287539c6d0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed = 6.8952 minutes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dtPredictions = fit_and_predict(DecisionTreeRegressor(seed = seed), train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265f935-72cd-4f27-8788-f8dfc4ffcb9d",
   "metadata": {
    "id": "b265f935-72cd-4f27-8788-f8dfc4ffcb9d"
   },
   "source": [
    "### 5. You are asked to forecast `fare` using `trip_seconds`, `trip_miles` and build a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1626e942-0cde-40b8-972d-8936b54bb653",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1626e942-0cde-40b8-972d-8936b54bb653",
    "outputId": "80c49294-96b9-4239-ffda-35ac3edbe3a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed = 10.76069 minutes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rfPredictions = fit_and_predict(RandomForestRegressor(seed = seed), train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f13309-0254-49b4-81b2-79b7591025fc",
   "metadata": {
    "id": "b5f13309-0254-49b4-81b2-79b7591025fc"
   },
   "source": [
    "### 6. You are asked to forecast `fare` using `trip_seconds`, `trip_miles` and build a gradient-boosted tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efce30ac-ef70-439f-bd5a-5db88c18183f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efce30ac-ef70-439f-bd5a-5db88c18183f",
    "outputId": "f88cfc81-bd12-4de6-f686-a9662bb37040"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed = 1.1932 hours\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbtPredictions = fit_and_predict(GBTRegressor(seed = seed), train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f74af1c-7306-450d-842b-7cbe91646740",
   "metadata": {
    "id": "1f74af1c-7306-450d-842b-7cbe91646740"
   },
   "source": [
    "### 7. Which model do you recommend? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a2f943-f729-451f-8cd7-566f6eb27d32",
   "metadata": {
    "id": "00a2f943-f729-451f-8cd7-566f6eb27d32"
   },
   "source": [
    "**I recommend the decision tree model.**\n",
    "\n",
    "The linear regression model is slightly worse than the other models based on RMSE and quite a bit worse based on $R^2$.  The other three models perform similarly according to both RMSE and $R^2$ with the gradient-boosted tree model performing slightly better than the decision tree and random forest models according to both of these metrics.  \n",
    "\n",
    "The decision tree model is simpler than both the random forest and gradient-boosted tree models, so I would recommend this model.  A simpler model is particularly helpful when the dataset is as large as the one in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b20fa-7de9-4e98-b98c-6c7acb776141",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "469b20fa-7de9-4e98-b98c-6c7acb776141",
    "outputId": "26d941eb-1f20-4762-f9ff-c3ca9c45bcb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------+-------+\n",
      "|model                |RMSE    |R2     |\n",
      "+---------------------+--------+-------+\n",
      "|GBTRegressor         |23.25334|0.19844|\n",
      "|RandomForestRegressor|23.35318|0.19154|\n",
      "|DecisionTreeRegressor|23.35943|0.19111|\n",
      "|LinearRegression     |24.86094|0.08378|\n",
      "+---------------------+--------+-------+\n",
      "\n",
      "time elapsed = 13.67705 minutes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "t0_evaluate = time.time()\n",
    "\n",
    "# create a DataFrame with metrics (RMSE and R^2) for each model\n",
    "metrics = []\n",
    "models = [\"LinearRegression\", \"DecisionTreeRegressor\", \"RandomForestRegressor\", \"GBTRegressor\"]\n",
    "predictions = [lrPredictions, dtPredictions, rfPredictions, gbtPredictions]\n",
    "for i in range(len(models)):\n",
    "    evaluator = RegressionEvaluator()\n",
    "    model_predict = predictions[i]\n",
    "    rmse = round(evaluator.evaluate(model_predict, {evaluator.metricName: \"rmse\"}), 5)\n",
    "    r2 = round(evaluator.evaluate(model_predict, {evaluator.metricName: \"r2\"}), 5)\n",
    "    metrics.append([models[i], rmse, r2])\n",
    "metrics_df = spark.createDataFrame(metrics, [\"model\", \"RMSE\", \"R2\"])\n",
    "# sort the metrics DataFrame by RMSE\n",
    "metrics_df.sort([\"RMSE\"], ascending = True).show(truncate = False)\n",
    "\n",
    "print(find_time_elapsed(t0_evaluate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdf4290-f1b9-49d2-92fe-426b66c89334",
   "metadata": {
    "id": "bbdf4290-f1b9-49d2-92fe-426b66c89334"
   },
   "source": [
    "### 8. Please perform hyper parameter tuning on the model you selected in step 7. (Since it is a huge dataset, you may use 1 parameter for each hyperparameter to save time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d33152-cbdf-4b64-b0b0-31ff221f6197",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14d33152-cbdf-4b64-b0b0-31ff221f6197",
    "outputId": "e2640e71-70fb-41e3-8272-d61677bcb12b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: DecisionTreeRegressionModel: uid=DecisionTreeRegressor_8551eaf5fbe3, depth=10, numNodes=631, numFeatures=2 \n",
      "\n",
      "minInstancesPerNode = 4 \n",
      "\n",
      "maxDepth = 10 \n",
      "\n",
      "time elapsed = 56.94527 minutes\n"
     ]
    }
   ],
   "source": [
    "from pandas.io.formats.info import Dtype\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "t0_cv = time.time()\n",
    "\n",
    "# use the model selected in step 7\n",
    "dt = DecisionTreeRegressor(seed = seed)\n",
    "# create a pipeline with one stage (the model)\n",
    "pipeline = Pipeline(stages = [dt])\n",
    "\n",
    "parameters = ParamGridBuilder() \\\n",
    "                .addGrid(dt.minInstancesPerNode, [1, 2, 4]) \\\n",
    "                .addGrid(dt.maxDepth, [5, 10]) \\\n",
    "                .build()\n",
    "\n",
    "# perform cross validation to find the best parameters\n",
    "cv = CrossValidator(estimator = pipeline, estimatorParamMaps = parameters, evaluator = RegressionEvaluator(), numFolds = 3, seed = seed)\n",
    "cv_model = cv.fit(train)\n",
    "print(\"best model:\", cv_model.bestModel.stages[-1], \"\\n\")\n",
    "print(\"minInstancesPerNode =\", cv_model.bestModel.stages[-1].getMinInstancesPerNode(), \"\\n\")\n",
    "print(\"maxDepth =\", cv_model.bestModel.stages[-1].getMaxDepth(), \"\\n\")\n",
    "\n",
    "print(find_time_elapsed(t0_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0026cb-1db6-41da-8d38-3d7fdac1e5b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca0026cb-1db6-41da-8d38-3d7fdac1e5b6",
    "outputId": "f10b43ed-5099-4a38-f0a8-7ccc5fc6116d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 23.24027\n",
      "R^2 = 0.19934\n"
     ]
    }
   ],
   "source": [
    "# evaluate the best model (find RMSE and R^2)\n",
    "evaluator = RegressionEvaluator()\n",
    "predictions = cv_model.bestModel.transform(test)\n",
    "rmse = round(evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"}), 5)\n",
    "print(\"RMSE =\", rmse)\n",
    "r2 = round(evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"}), 5)\n",
    "print(\"R^2 =\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YIH_acAZ6DiB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YIH_acAZ6DiB",
    "outputId": "a36446ec-5452-4b18-a452-e764debd2f4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|  label|        prediction|\n",
      "+-------+------------------+\n",
      "|9500.45|  40.2066219178905|\n",
      "|9476.21| 57.00532193803531|\n",
      "|9300.45| 35.75285125770721|\n",
      "|9276.62| 57.00532193803531|\n",
      "|9026.31| 26.85222517245868|\n",
      "|9002.29| 37.87905074626866|\n",
      "|9001.52|15.904370564212362|\n",
      "|9001.17| 9.123955842988858|\n",
      "| 9001.0|12.069101031367095|\n",
      "|9000.62|7.2128795830498325|\n",
      "+-------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display the predictions for the 10 largest labels\n",
    "# these predictions are very off and are increasing the RMSE\n",
    "predictions.sort([\"label\"], ascending = False).select([\"label\", \"prediction\"]).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c23cb22-a622-461f-8b26-d9a89ab34185",
   "metadata": {
    "id": "3c23cb22-a622-461f-8b26-d9a89ab34185"
   },
   "source": [
    "### 9. Perform k-means clustering using features `trip_seconds`, `trip_miles`, and `fare`. Please recommend the optimal number of clusters and justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f18fb3-e337-41ca-af8b-551453878ef4",
   "metadata": {
    "id": "c9f18fb3-e337-41ca-af8b-551453878ef4"
   },
   "source": [
    "**I would recommend 5 clusters.**\n",
    "\n",
    "The best silhouette value results from k = 2, and the second-best silhouette value results from k = 5.\n",
    "\n",
    "Considering that the training data contains almost 14 million observations, I would recommend 5 clusters.  Clustering a dataset this large into 5 clusters seems more reasonable than clustering it into only 2 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae435f-dec9-403c-b83d-8556e4f9ff02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfae435f-dec9-403c-b83d-8556e4f9ff02",
    "outputId": "f76a7262-d921-4864-e75f-f9285ef2eb8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------+-----------------------+---------------------------------------------------------------+\n",
      "|label  |trip_seconds|trip_miles|k_means_features       |k_means_features_scaled                                        |\n",
      "+-------+------------+----------+-----------------------+---------------------------------------------------------------+\n",
      "|9999.0 |12          |0.0       |[12.0,0.0,9999.0]      |[1.388904964177826E-4,0.0,1.0]                                 |\n",
      "|9890.12|11820       |161.6     |[11820.0,161.6,9890.12]|[0.13680713897151586,0.048194208344517014,0.9891109110911093]  |\n",
      "|9800.45|1980        |0.0       |[1980.0,0.0,9800.45]   |[0.02291693190893413,0.0,0.9801430143014302]                   |\n",
      "|9739.58|1140        |716.0     |[1140.0,716.0,9739.58] |[0.013194597159689347,0.2135337448927858,0.974055405540554]    |\n",
      "|9600.48|3780        |0.0       |[3780.0,0.0,9600.48]   |[0.04375050637160152,0.0,0.9601440144014401]                   |\n",
      "|9500.45|2220        |0.0       |[2220.0,0.0,9500.45]   |[0.025694741837289783,0.0,0.9501400140014002]                  |\n",
      "|9490.61|300         |91.8      |[300.0,91.8,9490.61]   |[0.0034722624104445653,0.027377650532343208,0.9491559155915592]|\n",
      "|9400.24|960         |0.0       |[960.0,0.0,9400.24]    |[0.01111123971342261,0.0,0.9401180118011802]                   |\n",
      "|9200.46|3240        |0.0       |[3240.0,0.0,9200.46]   |[0.0375004340328013,0.0,0.92013801380138]                      |\n",
      "|9200.45|3600        |0.0       |[3600.0,0.0,9200.45]   |[0.04166714892533478,0.0,0.9201370137013702]                   |\n",
      "+-------+------------+----------+-----------------------+---------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# first, assemble the \"trip_seconds\", \"trip_miles\", and \"label\" (\"fare\") columns into a \"k_means_features\" column using VectorAssembler\n",
    "assembler = VectorAssembler(inputCols = [\"trip_seconds\", \"trip_miles\", \"label\"], outputCol = \"k_means_features\")\n",
    "train_k_means = assembler.transform(train)\n",
    "\n",
    "# second, scale the \"k_means_features\" column so that each feature value is between 0 and 1\n",
    "scaler = MinMaxScaler(inputCol = \"k_means_features\", outputCol = \"k_means_features_scaled\", min = 0, max = 1)\n",
    "train_k_means = scaler.fit(train_k_means).transform(train_k_means)\n",
    "train_k_means = train_k_means.select([\"label\", \"trip_seconds\", \"trip_miles\", \"k_means_features\", \"k_means_features_scaled\"])\n",
    "train_k_means.sort([\"label\"], ascending = False).show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v3R5_O6rBTln",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v3R5_O6rBTln",
    "outputId": "75011958-4029-47c6-e890-6e82a9292ff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|  k|        silhouette|\n",
      "+---+------------------+\n",
      "|  2| 0.998739393565863|\n",
      "|  3|0.7994544107024001|\n",
      "|  4|0.7236816173458225|\n",
      "|  5|0.8400285858935831|\n",
      "|  6|0.7468181144931019|\n",
      "|  7|0.6423728508376305|\n",
      "|  8|0.7398369725001089|\n",
      "|  9|0.6827657265896455|\n",
      "| 10|0.6249826754905856|\n",
      "| 11|0.6504570983637393|\n",
      "+---+------------------+\n",
      "\n",
      "time elapsed = 3.30081 hours\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "t0_k_means = time.time()\n",
    "\n",
    "silhouettes = []\n",
    "for k in range(2, 12):\n",
    "    # create, fit, and transform the k-means model\n",
    "    kmeans = KMeans(featuresCol = \"k_means_features_scaled\").setK(k).setSeed(seed)\n",
    "    predictions = kmeans.fit(train_k_means).transform(train_k_means)\n",
    "    \n",
    "    # evaluate the k-means model\n",
    "    evaluator = ClusteringEvaluator(featuresCol = \"k_means_features_scaled\")\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    silhouettes.append([k, silhouette])\n",
    "    \n",
    "# create a DataFrame with the silhouette values corresponding to different values of k\n",
    "silhouettes_df = spark.createDataFrame(silhouettes, [\"k\", \"silhouette\"])\n",
    "silhouettes_df.sort([\"k\"], ascending = True).show()\n",
    "\n",
    "print(find_time_elapsed(t0_k_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f94fb-21fe-4b20-8b41-30c5e6e1e6bb",
   "metadata": {
    "id": "742f94fb-21fe-4b20-8b41-30c5e6e1e6bb"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
